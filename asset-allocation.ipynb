{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the disadvantages of Markowitz or Mean-Variance analysis in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A most straightforward approach is to use sample mean and sample covariance in place of the theoretical counterparts. But it is **hard to estimate** $\\mu$ and $\\Sigma$.\n",
    "    - Note that to get an **unbiased estimate** of the optimal portfolio weights, one actually **cannot use the sample covariance**, but there is a multiplicative adjustment; see Page 4 of Lecture 9 in Steve Kou's lecture notes.\n",
    "- To make matters worse, the optimal portfolio weights are **very sensitive to estimation errors** in $\\mu$ and $\\Sigma$.\n",
    "    - Even if $\\Sigma$ is known, it can be shown that the error between sample optimal weight and the true optimal weight can still be **enlarged by the conditional number of $\\Sigma$**, which in turn is large **when there is a large number of assets in the universe and small number of observations**.\n",
    "    - Also, the estimated, or sample optimal weights are not robust, in the sense that **even small change in the sample mean of just one asset could lead to significant changes in the sample optimal weights, sometimes even changing the signs**; see Page 6 of Lecture 9 for the two points above.\n",
    "    - Some suggest it is more important to try to estimate the mean accurately than the covariance, but that could **hinges on the assumption of i.i.d. normality: the fatter the tails are, the more difficult to estimate second moments, since there will be more outliers**.\n",
    "- The mean-variance theory is based on the quadratic utility function, which itself is problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some possible improvements of the estimation errors in sample mean and sample variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A significant improvement over the sample mean vector and sample covariance can be achieved by using **shrinkage method**. The basic idea is to shrink the mean vector and the covariance matrix toward a plausible value: **though this introduces bias, it can more significantly reduce the variance part of the estimation error**, which hopefully ends up reducing total estimation error.\n",
    "\n",
    "    - Charles Stein (1956) started the line of research by showing that if the distribution is multivariate normal distribution with $\\Sigma = I$ and the dimensional $N\\geq 3$, then there exist an estimator whose mean square estimation error is strictly smaller than that of the sample mean vector. \n",
    "    - Pursuing the logic above further, the **James-Stein estimator** can be constructed. It is an **empirical Bayes estimator and is a shrinkage between the sample mean and the overall average for all $N$ assets**. \n",
    "        - The intuition of this approach is that, **returns which are a long way from the norm have a higher chance of containing measurement error than those close to it**. Thus estimates based on individual share data are cross-sectionally 'shrunk' towards an overall estimate of expected returns based on all the data.\n",
    "        - Again, such estimates are nonlinear biased estimators, but have the attractive property of a smaller total risk than ML estimate when the observations come from a multivariate normal distribution with a known covariance matrix. \n",
    "            - For large sample sizes the inadmissibility problem disappears. and is of no concern. Also, it will not be a troublesome problem if the number of stocks $N$ is very small. The major difficulties arise for large $N$ and small sample size $T$.\n",
    "    - Other Stein estimators are available for the mean-variance optimization. \n",
    "        - **In mean**. \n",
    "            - Jorion (1986) shrinks the sample mean toward the **mean of the global minimum variance portfolio**. Jorion shows in an out-of sample test that the classical approach is systematically outperformed by a strategy based on the shrinkage estimators.\n",
    "            - Other forms of shrinkage are possible, for instance **shrinking toward the CAPM expected returns**; this is what is in the Black-Litterman model. \n",
    "            - For small sample sizes (n < 60) , Jobson and Korkie suggest **shrinking towards the grand mean or even set all expected values to the same value**.\n",
    "        - **In mean and covariance together**. Frost and Savarino (1986) produces a joint estimator of the means and covariances in an unified framework (to go into more detail).\n",
    "        - **In covariance**. \n",
    "            - The estimation of covariance matrix is reasonable with low dimension $N$ and large sample size $T$. However, with large $N$, the covariance matrix quickly between **ill-conditioned**, or has near zero determinant, or has large condition number (the ratio between the largest and smallest eigenvalue): all mean the same thing. Ill-condition ascerbates estimation error even when $T$ is little more than $N$.\n",
    "            - In order to solve the ill-conditioning problem of the covariance matrix, Ledoit (1999) suggests to estimate it by using a shrinkage method, more precisely, by **using an optimally weighted average of two existing estimators, the single index covariance matrix via a factor regression and the sample covariance matrix**. This choice has some advantages: \n",
    "                - There is some consensus on representing high-dimensional covariance matrix by a few factors and their structure; for example many people use CAPM as a single index factor model, or Fama-French three-factor model (relatedly, in his [presidential address](https://www.evernote.com/shard/s191/nl/21353936/454d91df-398b-4080-b7ec-aa7b6adbca4d?title=Presidential%20Address:%20Discount%20Rates), John Cochrane evens describes covariance as 'Fama-French central result'). \n",
    "                - It even allows estimation of the covariance matrix even if the number of assets is greater than the number of observations, as it also uses the information from the factor model. \n",
    "                - Although shrinking toward a low-rank factor structure could potentially introduce bias (the factor structure could be wrong!), as other Stein estimator, the point is to reduce overall estimation error by reducing variance. \n",
    "            - Fan, Fan and Lv (2006) have examined the advantage of using a factor structure in estimating the covariance matrix. \n",
    "                - They show that the factor matrix is **always invertible**, while the sample matrix suffers from the problem of possibly being singular when dimensionality is close to or larger than sample size n. \n",
    "                - The advantage of the factor model lies in the estimation of the **inverse of the covariance matrix**, **not the estimation of the covariance matrix itself**. When the parameters involve the inverse of the covariance matrix, the factor model shows substantial gains, whereas when the parameters involved the covariance matrix directly, the factor model does not have much advantage. This has different practical implications. \n",
    "                    - First, since portfolio allocations involve the inverse of the covariance matrix, the factor-model based estimates gain substantially. \n",
    "                    - Secondly, because many risk management practise only involve directly the covariance matrix, the gain there may be only marginally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bayesian method**: Black-Litterman model (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Putting **constraints on portfolio weights**, such as upper and lower bounds, so that they are reasonsable. Sometimes this can help reduce the out-of-sample volatility in the investor's optimal portfolio weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally in practice many people use resampling procedure, which means resampling the historical data to compute optimal weights for each resampling data set, and then **compute the average optimal weights**. However, there are serious pitfalls in the portfolio resampling procedure, as it has almost no decision theoretic foundation, and it is just another heuristic. In particular, **all re-sampled portfolios inherit the same estimation error from the original data, and averaging may not help much**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Black-Litterman Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black and Litterman propose a Bayesian model that tries to solve the problems we have with the mean variance optimizer that (1) portfolios are often highly leveraged, and (2) portfolio allocations change drastically with small changes in the forecasts.\n",
    "\n",
    "Let $X$ be the excess returns of $N$ risky assets, Black-Litterman has three components\n",
    "- Black-Litterman assumes normality for $X$ and its covariance matrix $\\Sigma$ is known:\n",
    "  \\begin{align}\n",
    "  X|\\mu \\sim N(\\mu, \\Sigma).\n",
    "  \\end{align}\n",
    "  The assumption is based on (1) the belief that it is much harder to estimate the mean than the covariance matrix, and (2) the fact that assuming normality, the impact of the mean to the optimal asset allocation is much more than that of the covariance matrix.\n",
    "- Black-Litterman assumes $\\mu$ has a prior distribution:\n",
    "  \\begin{align}\n",
    "  \\mu\\sim N(\\pi, C),\n",
    "  \\end{align}\n",
    "   - $\\pi$ is backed out from the market portfolio capitalization weight, where the market portfolio is assumed to be a mean-variance efficient weight and in equilibrium. More specifically, $\\pi$ is a function of the (estimated) market expected return and standard deviation, and risk-free rate. **Question: it may be straightforward if the portfolio is consist of stocks and bonds; but the model will need to be adapted if it also include currencies and options: in Black-Litterman's original paper, currencies are included, and assumptions were made but did not quite get the details yet.**\n",
    "   - $C$ is suggested to be just a multiple of $\\Sigma$: $C = \\tau\\Sigma$, where $\\tau$ is a lever whereby small/big $\\tau$ indicates strong/weak confidence in the market equilibrium. \n",
    "   - Empirically, since market capitalization weight is typically slow moving, and the estimated $\\Sigma$ tends to be more stable (at least than the estimated $\\mu$), this prior distribution serves as an anchor.\n",
    "- The subjective views are given in a linear form\n",
    "  \\begin{align}\n",
    "  P\\mu = q + \\epsilon,\n",
    "  \\end{align}\n",
    "  where $\\epsilon$ is a noise vector with $\\epsilon\\sim N(0, \\Omega)$. \n",
    "   - $P$ and $q$ are specified such that portfolio manager's view is specified: it can be either absolute view or relative view. \n",
    "   - $\\Omega$ is used to specify how confident the manager is about a view; typically $\\Omega$ is specified as a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the model, in a Bayesian perspective, is to treat the first two points above as 'prior' on $X$ and $\\mu$ and the subject views as 'data' on $\\mu$, then solve for the 'posterior' mean and variance of $X$: we only need the posterior mean and variance since we are then using the mean and variance to construct the optimal portfolio weight just as the traditional mean-variance analysis will do.\n",
    "\n",
    "Intuitively, without the subjective views, the unconstrained optimal portfolio in the Black-Litterman model is the scaled market equilibrium portfolio (reflecting the uncertainty in the equilibrium expected returns). The subjective views give rise to a weighted sum of portfolios as add-on representing the investors' views. \n",
    "- The weight on a portfolio representing a view is positive when the view is more bullish than the one implied by the equilibrium and the other views. \n",
    "- The weight increases as the investor becomes more bullish on the view, and the magnitude of the weight also increases as the investor becomes more confident about the view.\n",
    "\n",
    "In this way, Black-Litterman incorporates investor's view in a systematic way into the final asset allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Steve Kou's Lecture notes, Lecture 9 and 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
